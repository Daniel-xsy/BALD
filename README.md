<p align="center">
  <h3 align="center">  
    <strong>Can We Trust Embodied Agents? <br>Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems</strong>
  </h3>

<p align="center">
    <a href="https://jrcblue.github.io" target='_blank'>Ruochen Jiao*</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://daniel-xsy.github.io/" target='_blank'>Shaoyuan Xie*</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://scholar.google.com/citations?user=miXKtPUAAAAJ&hl=en" target='_blank'>Justin Yue</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://tkm2261.github.io" target='_blank'>Takami Sato</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;<br>
    <a href="https://conditionwang.github.io" target='_blank'>Lixu Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://wangyixu14.github.io" target='_blank'>Yixuan Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://ics.uci.edu/~alfchen/" target='_blank'>Qi Alfred Chen</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="http://zhulab.ece.northwestern.edu/index.html" target='_blank'>Qi Zhu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    </br></br>
    <sup>1</sup>Northwestern University&nbsp;&nbsp;&nbsp;&nbsp;
    <sup>2</sup>University of California, Irvine&nbsp;&nbsp;&nbsp;&nbsp;
    <br>
    *Equal contribution
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2405.20774" target='_blank'>
    <img src="https://img.shields.io/badge/Paper-%F0%9F%93%83-lightblue">
  </a>&nbsp;
  <a href="" target='_blank'>
    <img src="https://img.shields.io/badge/Dataset-%F0%9F%8E%AC-pink">
  </a>&nbsp;
</p>


## Overview

Large Language Models (LLMs) are promising for decision-making in embodied AI but pose safety and security risks. We introduce BALD, a framework for Backdoor Attacks on LLM-based systems, exploring attack surfaces and triggers. We propose three attack mechanisms: word injection, scenario manipulation, and knowledge injection. Our experiments on GPT-3.5, LLaMA2, and PaLM2 in autonomous driving and home robot tasks show high success rates and stealthiness. Our findings highlight critical vulnerabilities and the need for robust defenses in embodied LLM systems.

<p align="center">
    <img src="./assets/teaser.png" alt="Teaser Figure" width="600">
</p>

## Experiments

Comming soon.
